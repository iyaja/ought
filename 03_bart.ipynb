{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "political-morris",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp bart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "czech-violence",
   "metadata": {},
   "source": [
    "# BART Zero-Shot Prediction Classification \n",
    "\n",
    "> Using modern zero-shot classification techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "center-postcard",
   "metadata": {},
   "source": [
    "The starter code is helpful, but Huggingface has built-in tools for zero shot classification. This also makes it easier to test different models (some may be trained on a larger amount of scientific text, which will be helpful)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "other-celebrity",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from ought.starter import *\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-nancy",
   "metadata": {},
   "source": [
    "Here, we'll use FaceBook's [BART](https://huggingface.co/facebook/bart-large-mnli) model. It was explicitly designed for zero-shot text classification (among other tasks), and should work well out of the box. The same prompts as GPT-2 are used for consistency, though there is potentially some scope for tuning here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-falls",
   "metadata": {},
   "outputs": [],
   "source": [
    "clas = pipeline(\"zero-shot-classification\", device=0)\n",
    "labels = [\"AI\", \"Not AI\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foreign-enforcement",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = load_jsonl(\"data/train.jsonl\")\n",
    "prompt = make_prompt('Label each of the following examples as \"AI\" or \"NOT AI\"', samples[:5], samples[5])\n",
    "pred = clas(prompt, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fantastic-yugoslavia",
   "metadata": {},
   "source": [
    "## Refactor into a Single Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-banner",
   "metadata": {},
   "source": [
    "We can refactor all this and export it as a single class with two useful methods:\n",
    "\n",
    "- An initializer that will retrain a new model for *every* new instance. This is intended, since we do not know the training set ahead of time. One potential improvement here would be to continuously train on every new `.jsonl` file that comes in and save the weights, but there is not enough data for that here. \n",
    "- A `predict` method that takes in a sentence and returns a prediction by querying the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-performer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class BARTClassifier:\n",
    "    def __init__(self, instructions='Label each of the following examples as \"AI\" or \"NOT AI\"', json='data/train.jsonl', samples=2):\n",
    "        self.instructions = instructions\n",
    "        self.context = uniform_samples(json, samples)\n",
    "        self.clas = pipeline(\"zero-shot-classification\", device=0)\n",
    "        self.labels = [\"AI\", \"Not AI\"]\n",
    "        \n",
    "    def predict(self, prompt):\n",
    "        prompt = make_prompt(self.instructions, self.context, {'text': prompt})\n",
    "        # to create a concrete prediction, take the last line and strip the \"LABEL: \" component \n",
    "        result = self.clas(prompt, self.labels)\n",
    "        pred = 'Not AI' if result['scores'][0] > result['scores'][1] else 'AI' \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crazy-naples",
   "metadata": {},
   "source": [
    "> Note: you might have to restart the notebook to clear GPU memory at this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continent-panama",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'out of plane effect on the superconductivity of sr2 xbaxcuo3+d with tc up to 98k. we comment on the paper published by w.b. gao q.q. liu l.x. yang y.yu f.y. li c.q. jin and s. uchida in phys. rev. b and give alternate explanations for the enhanced superconductivity. the enhanced onset tc of 98k observed upon substituting ba for sr is attributed to optimal oxygen ordering rather than to the increase in volume. comparison with la2cuo +x samples suggest that the effect of disorder is overestimated.'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = load_jsonl(\"data/test_no_labels.jsonl\")\n",
    "example = test[0]\n",
    "prompt = example['text']\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-stake",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/bart-large-mnli were not used when initializing BartModel: ['model.encoder.version', 'model.decoder.version']\n",
      "- This IS expected if you are initializing BartModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at facebook/bart-large-mnli were not used when initializing BartForSequenceClassification: ['model.encoder.version', 'model.decoder.version']\n",
      "- This IS expected if you are initializing BartForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.9 s, sys: 1.16 s, total: 24.1 s\n",
      "Wall time: 24.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clas = BARTClassifier()\n",
    "pred = clas.predict(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinguished-industry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Not AI'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
