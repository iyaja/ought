{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp ought"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ought Text Classification Project\n",
    "\n",
    "> Binary classification on scientific paper abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-2 Prompt Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest approach is to to feed in a small number of trainging examples into the prompt directly for zero-shot classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.eval().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def generate(prompt, max_length=5, stop_token=None):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    generated_text_ids = model.generate(input_ids=input_ids.cuda(), max_length=max_length+len(input_ids[0]), do_sample=False)\n",
    "    generated_text = tokenizer.decode(generated_text_ids[0], clean_up_tokenization_spaces=True)\n",
    "    post_prompt_text = generated_text[len(tokenizer.decode(input_ids[0], clean_up_tokenization_spaces=True)):]\n",
    "    return prompt + post_prompt_text[:post_prompt_text.find(stop_token) if stop_token else None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the logits are shifted over 1 to the left, since HuggingFace doesn't give a logit for the first token\n",
    "def get_logits_and_tokens(text):\n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    tokens = [tokenizer.decode([input_id]) for input_id in input_ids[0]]\n",
    "    output = model(input_ids.cuda())\n",
    "    return output.logits[0][:-1], tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLE_PROMPT = \"\"\"Horrible: negative\n",
    "Great: positive\n",
    "Bad:\"\"\"\n",
    "\n",
    "generated_text = generate(EXAMPLE_PROMPT, stop_token=\"\\n\")\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, tokens = get_logits_and_tokens(generated_text)\n",
    "last_token_probs = torch.softmax(logits[-1], dim=0)\n",
    "negative_prob = last_token_probs[tokenizer.encode(\" negative\")[0]]\n",
    "positive_prob = last_token_probs[tokenizer.encode(\" positive\")[0]]\n",
    "\n",
    "print(f\"tokens: {tokens}\\nnegative prob: {negative_prob}\\npositive prob: {positive_prob}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define helper function to load text from `.jsonl` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load_jsonl(filename):\n",
    "    f = open(filename)\n",
    "    return [json.loads(line) for line in f.read().splitlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = load_jsonl(\"data/train.jsonl\")\n",
    "train_examples[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def render_example(example):\n",
    "    title = example[\"text\"].split(\".\")[0].strip()\n",
    "    abstract = example[\"text\"][len(title)+1:].strip()\n",
    "    return f\"\"\"Title: {title}\n",
    "Abstract: {abstract}\n",
    "Label: {\"AI\" if example[\"label\"] == \"True\" else \"Not AI\"}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def render_end_example(example):\n",
    "    title = example[\"text\"].split(\".\")[0].strip()\n",
    "    abstract = example[\"text\"][len(title)+1:].strip()\n",
    "    return f\"\"\"Title: {title}\n",
    "Abstract: {abstract}\n",
    "Label:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def make_prompt(instructions, train_examples, end_example):\n",
    "    rendered_train_examples = \"\\n\\n--\\n\\n\".join([render_example(example) for example in train_examples])\n",
    "    return f\"\"\"{instructions}\n",
    "\n",
    "{rendered_train_examples}\n",
    "\n",
    "--\n",
    "\n",
    "{render_end_example(end_example)}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTRUCTIONS = \"Classify the following examples based on whether they are AI-relevant or not:\"\n",
    "\n",
    "prompt = make_prompt(INSTRUCTIONS, train_examples[:4], train_examples[4])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = generate(prompt, stop_token=\"\\n\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt engineering can only get you so far, and is not guaranteed to generate valid labels. It _should_ be possible to retrain a network using a small number of examples in under a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
