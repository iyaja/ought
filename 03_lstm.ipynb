{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "capable-statistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp lstm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cordless-slovenia",
   "metadata": {},
   "source": [
    "# Few-Shot LSTM Fine-Tuning\n",
    "\n",
    "> Training an LSTM from scratch at test time for a single prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desperate-dominican",
   "metadata": {},
   "source": [
    "It should be possible to fine-tune at \"runtime\" with small number of examples from the training set.\n",
    "\n",
    "Since the total training time must be under a minute, the model *cannot* be unreasonably large like BERT or GPT. Considering this, we'll train much simpler and smaller models that are known to have better convergence properties. This is important because we don't what the \"training\" data is going to be ahead of time, since this is user-supplied. Also, the model needs to be able to generalize from a small number of examples, for which large transformers may not be the best option.\n",
    "\n",
    "`fastai` is well-suited for this rapid training where convergence across many samples with minimal configuration is more important than stictly obtaining the highest possible accuracy on the runtime training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "annoying-today",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from ought.starter import *\n",
    "import fastai\n",
    "from fastai.text.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-thumbnail",
   "metadata": {},
   "source": [
    "`fastai`is easy to work with when you adhere to their `DataLoaders` format. So first, convert the JSON data into a pandas `DataFrame`\n",
    "\n",
    "> Note: `fastai` handles importing common libraries like `pandas` and `matplotlib` under the usual namespaces, which is why you won't see those here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "united-paste",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>meta</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>thermodynamic analysis of quantum error correcting engines. quantum error correcting codes can be cast in a way which is strikingly similar to a quantum heat engine undergoing an otto cycle. in this paper we strengthen this connection further by carrying out a complete assessment of the thermodynamic properties of strokes operator based error correcting codes. this includes an expression for the entropy production in the cycle which as we show contains clear contributions stemming from the different sources of irreversibility. to illustrate our results we study a classical qubit error corr...</td>\n",
       "      <td>{'id': '1911.06354', 'year': 2019}</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>nlo qcd corrections to wzjj production at the lhc. we present a summary of the first calculation of nlo qcd corrections to wzjj production with leptonic decays at the lhc. our results show that the next to leading order corrections reduce significantly the scale uncertainties.</td>\n",
       "      <td>{'id': '1310.4369', 'year': 2013}</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>asymptotics for lipschitz percolation above tilted planes. we consider lipschitz percolation in dimensions above planes tilted by an angle along one or several coordinate axes. in particular we are interested in the asymptotics of the critical probability as as well as our principal results show that the convergence of the critical probability to is polynomial as and in addition we identify the correct order of this polynomial convergence and in we also obtain the correct prefactor.</td>\n",
       "      <td>{'id': '1504.05405', 'year': 2015}</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>the colored jones polynomials for bridge links. kuperberg introduced web spaces for some lie algebras which are generalizations of the kauffman bracket skein module on a disk with marked points. we derive some formulas for and clasped web spaces by graphical calculus using skein theory. these formulas are colored version of skein relations twist formulas and bubble skein expansion formulas. we calculate the and colored jones polynomials of bridge knots and links explicitly using twist formulas.</td>\n",
       "      <td>{'id': '1609.07289', 'year': 2016}</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>population mixtures and searches of lensed and extended quasars across photometric surveys. wide field photometric surveys enable searches of rare yet interesting objects such as strongly lensed quasars or quasars with a bright host galaxy. past searches for lensed quasars based on their optical and near infrared properties have relied on photometric cuts and spectroscopic pre selection as in the sloan quasar lens search or neural networks applied to photometric samples. these methods rely on cuts in morphology and colours with the risk of losing many interesting objects due to scatter in ...</td>\n",
       "      <td>{'id': '1612.03821', 'year': 2016}</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  \\\n",
       "0  False   \n",
       "1  False   \n",
       "2  False   \n",
       "3  False   \n",
       "4  False   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "0  thermodynamic analysis of quantum error correcting engines. quantum error correcting codes can be cast in a way which is strikingly similar to a quantum heat engine undergoing an otto cycle. in this paper we strengthen this connection further by carrying out a complete assessment of the thermodynamic properties of strokes operator based error correcting codes. this includes an expression for the entropy production in the cycle which as we show contains clear contributions stemming from the different sources of irreversibility. to illustrate our results we study a classical qubit error corr...   \n",
       "1                                                                                                                                                                                                                                                                                                                                    nlo qcd corrections to wzjj production at the lhc. we present a summary of the first calculation of nlo qcd corrections to wzjj production with leptonic decays at the lhc. our results show that the next to leading order corrections reduce significantly the scale uncertainties.   \n",
       "2                                                                                                                  asymptotics for lipschitz percolation above tilted planes. we consider lipschitz percolation in dimensions above planes tilted by an angle along one or several coordinate axes. in particular we are interested in the asymptotics of the critical probability as as well as our principal results show that the convergence of the critical probability to is polynomial as and in addition we identify the correct order of this polynomial convergence and in we also obtain the correct prefactor.   \n",
       "3                                                                                                      the colored jones polynomials for bridge links. kuperberg introduced web spaces for some lie algebras which are generalizations of the kauffman bracket skein module on a disk with marked points. we derive some formulas for and clasped web spaces by graphical calculus using skein theory. these formulas are colored version of skein relations twist formulas and bubble skein expansion formulas. we calculate the and colored jones polynomials of bridge knots and links explicitly using twist formulas.   \n",
       "4  population mixtures and searches of lensed and extended quasars across photometric surveys. wide field photometric surveys enable searches of rare yet interesting objects such as strongly lensed quasars or quasars with a bright host galaxy. past searches for lensed quasars based on their optical and near infrared properties have relied on photometric cuts and spectroscopic pre selection as in the sloan quasar lens search or neural networks applied to photometric samples. these methods rely on cuts in morphology and colours with the risk of losing many interesting objects due to scatter in ...   \n",
       "\n",
       "                                 meta  is_valid  \n",
       "0  {'id': '1911.06354', 'year': 2019}     False  \n",
       "1   {'id': '1310.4369', 'year': 2013}     False  \n",
       "2  {'id': '1504.05405', 'year': 2015}     False  \n",
       "3  {'id': '1609.07289', 'year': 2016}     False  \n",
       "4  {'id': '1612.03821', 'year': 2016}     False  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path('data/')\n",
    "\n",
    "train = load_jsonl('data/train.jsonl')\n",
    "valid = load_jsonl('data/dev.jsonl')\n",
    "\n",
    "train_df = pd.DataFrame(train)\n",
    "valid_df = pd.DataFrame(valid)\n",
    "\n",
    "train_df['is_valid'] = False\n",
    "valid_df['is_valid'] = True\n",
    "\n",
    "df = train_df.append(valid_df, ignore_index=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-distribution",
   "metadata": {},
   "source": [
    "Given the number of samples, it should be possible to fine tune both a langage model *and* a classifier in under a minute, but it's not clear if this is profitable. SO let's try both."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ambient-arena",
   "metadata": {},
   "source": [
    "## Fine-Tuning Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "exceptional-straight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iyaja/miniconda3/lib/python3.8/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos on xxunk s problem on the classifications of convex lattice polytopes . in xxunk . xxunk studied the classification problem for convex lattice polygons of given area . since then this problem and its analogues have been studied by b ar any xxunk xxunk xxunk xxunk and others . upper bounds for the numbers of non equivalent xxunk convex lattice polytopes of given volume or cardinality have been achieved . in</td>\n",
       "      <td>on xxunk s problem on the classifications of convex lattice polytopes . in xxunk . xxunk studied the classification problem for convex lattice polygons of given area . since then this problem and its analogues have been studied by b ar any xxunk xxunk xxunk xxunk and others . upper bounds for the numbers of non equivalent xxunk convex lattice polytopes of given volume or cardinality have been achieved . in this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>squeezing of atomic ensembles in free space we xxunk on unique features that arise in the nanofiber geometry including anisotropy of both the intensity and polarization of the guided modes . we use a first principles stochastic xxunk equation to model the squeezing as function of time in the presence of xxunk due to optical xxunk . we find a peak xxunk squeezing of ~ db is achievable with current technology for</td>\n",
       "      <td>of atomic ensembles in free space we xxunk on unique features that arise in the nanofiber geometry including anisotropy of both the intensity and polarization of the guided modes . we use a first principles stochastic xxunk equation to model the squeezing as function of time in the presence of xxunk due to optical xxunk . we find a peak xxunk squeezing of ~ db is achievable with current technology for ~</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>we also extend the factorized resummation of multipolar amplitudes to generic mass ratio non precessing spinning black holes . lastly in our study we employ new recently computed higher order post newtonian terms in several xxunk modes and compute explicit expressions for the half and one and half post newtonian contributions to the odd parity current and even parity odd xxunk respectively . those results can be used to build more accurate</td>\n",
       "      <td>also extend the factorized resummation of multipolar amplitudes to generic mass ratio non precessing spinning black holes . lastly in our study we employ new recently computed higher order post newtonian terms in several xxunk modes and compute explicit expressions for the half and one and half post newtonian contributions to the odd parity current and even parity odd xxunk respectively . those results can be used to build more accurate templates</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls_lm = TextDataLoaders.from_df(df, path=path, text_col='text', label_col='label', valid_col='is_valid', is_lm=True)\n",
    "dls_lm.show_batch(max_n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "african-company",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.814971</td>\n",
       "      <td>5.548374</td>\n",
       "      <td>0.199942</td>\n",
       "      <td>256.819672</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.507803</td>\n",
       "      <td>5.307096</td>\n",
       "      <td>0.203712</td>\n",
       "      <td>201.763550</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.354535</td>\n",
       "      <td>5.116339</td>\n",
       "      <td>0.212079</td>\n",
       "      <td>166.723907</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.215483</td>\n",
       "      <td>5.041849</td>\n",
       "      <td>0.218871</td>\n",
       "      <td>154.755920</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.104574</td>\n",
       "      <td>5.015539</td>\n",
       "      <td>0.221631</td>\n",
       "      <td>150.737381</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.026503</td>\n",
       "      <td>5.011507</td>\n",
       "      <td>0.221808</td>\n",
       "      <td>150.130814</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = language_model_learner(dls_lm, AWD_LSTM, drop_mult=0.5, metrics=[accuracy, Perplexity()], path=path, wd=0.1).to_fp16()\n",
    "learn.fine_tune(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "offshore-upgrade",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save_encoder('finetuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thick-heading",
   "metadata": {},
   "source": [
    "## Adding Classification Head to Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "small-father",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iyaja/miniconda3/lib/python3.8/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos evaluation of peak wall stress in an ascending thoracic aortic xxunk using fsi simulations effects of aortic stiffness and peripheral resistance . purpose . it has been reported xxunk that rupture or xxunk in thoracic aortic xxunk taa often occur due to xxunk which may be modelled with sudden increase of peripheral resistance inducing xxunk changes of blood volumes in the xxunk . there is clinical evidence that more compliant xxunk are less prone to rupture as they can xxunk such changes of volume . the aim of the current paper is to verify this paradigm by evaluating computationally the role played by the variation of peripheral resistance and the impact of aortic stiffness onto peak wall stress in ascending taa . methods . fluid structure interaction fsi analyses were performed using xxunk specific geometries and boundary conditions derived from 4d mri datasets acquired on a xxunk . blood</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xxbos grain opacity and the bulk composition of extrasolar planets . ii . an analytical model for the grain opacity in protoplanetary atmospheres . context . we investigate the grain opacity k gr in the atmosphere of xxunk . this is important for the planetary mass radius relation since k gr affects the h he envelope mass of low mass planets and the critical core mass of giant planets . aims . the goal of this study is to derive an analytical model for k gr . methods . our model is based on the comparison of the timescales of xxunk processes like grain settling in the stokes and xxunk regime growth by brownian motion xxunk and differential settling grain evaporation and grain xxunk due to envelope contraction . with these timescales we derive the grain size abundance and opacity . results . we find that the main growth process</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xxbos evaluating the applicability of the fokker planck equation in polymer translocation a brownian dynamics study . brownian dynamics xxunk simulations are used to study the translocation dynamics of a coarse grained polymer through a xxunk nanopore . we consider the case of short xxunk with a polymer length n in the range n= . the rate of translocation is controlled by a tunable friction coefficient gamma 0p for monomers inside the nanopore . in the case of xxunk translocation the mean translocation time scales with polymer length n as &lt; tau &gt; ~ n n p xxunk where n p is the average number of monomers in the nanopore . the exponent approaches the value alpha= when the pore friction is sufficiently high in xxunk with the prediction for the case of the quasi static regime where pore friction xxunk . in the case of xxunk translocation the polymer</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls = TextDataLoaders.from_df(df, path=\"data\", text_col='text', label_col='label', valid_col='is_valid', seq_len=50)\n",
    "dls.show_batch(max_n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abandoned-mechanics",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "invisible-treasurer",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = learn.load_encoder('finetuned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "supported-layout",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.708900</td>\n",
       "      <td>0.505493</td>\n",
       "      <td>0.892000</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.522991</td>\n",
       "      <td>0.342583</td>\n",
       "      <td>0.898000</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.391128</td>\n",
       "      <td>0.430593</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.316259</td>\n",
       "      <td>0.382648</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.251506</td>\n",
       "      <td>0.385069</td>\n",
       "      <td>0.894000</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.196436</td>\n",
       "      <td>0.336438</td>\n",
       "      <td>0.896000</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fine_tune(5, 5e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalid-victorian",
   "metadata": {},
   "source": [
    "## Fine-Tuning Classifier from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-burke",
   "metadata": {},
   "source": [
    "Now, we'll train the classifier on it's own and see if the performance is significantly worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "solved-allergy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iyaja/miniconda3/lib/python3.8/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.708454</td>\n",
       "      <td>0.406301</td>\n",
       "      <td>0.902000</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.592001</td>\n",
       "      <td>0.362658</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.439429</td>\n",
       "      <td>0.365592</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.323140</td>\n",
       "      <td>0.576635</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.249769</td>\n",
       "      <td>0.265709</td>\n",
       "      <td>0.914000</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.195983</td>\n",
       "      <td>0.269402</td>\n",
       "      <td>0.912000</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls = TextDataLoaders.from_df(df, path=\"data\", text_col='text', label_col='label', valid_col='is_valid', seq_len=50)\n",
    "learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\n",
    "learn.fine_tune(5, 5e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considered-passing",
   "metadata": {},
   "source": [
    "Surprisingly, fine-tuning the classifier on its own is better than fine-tuning the language model + classifier in this case. This is good news, since it means we can allocate more time to training the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "widespread-friday",
   "metadata": {},
   "source": [
    "Finally, as a sanity check, we can see some sample predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "developed-zoning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>category_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos the energetics of giant radio galaxy lobes from inverse compton scattering observations . giant radio galaxy grg lobes are excellent laboratories to study the evolution of the particle and b field energetics . however these results are based on assumptions of the shape and extension of the grg lobe electron spectrum . we re examine the energetics of grg lobes as derived by inverse compton scattering of cmb photons ics cmb by relativistic electrons in rg lobes to assess the physical conditions of rg lobes their energetics and their radiation regime . we consider the grg da recently observed by xxunk as a reference case and we also discuss other rg lobes observed with chandra and xxunk . we model the spectral energy distribution of the da xxunk lobe to get constraint on the shape and the extension of the electron spectrum in the lobe by using multi frequency</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xxbos asymptotic normality and xxunk in estimation of large gaussian graphical models . the gaussian graphical model a popular paradigm for studying relationship among variables in a wide range of applications has attracted great attention in recent years . this paper considers a fundamental question when is it possible to estimate low dimensional parameters at parametric square root rate in a large gaussian graphical model a novel regression approach is proposed to obtain asymptotically efficient estimation of each entry of a precision matrix under a xxunk condition relative to the sample size . when the precision matrix is not sufficiently sparse or xxunk the sample size is not sufficiently large a lower bound is established to show that it is no longer possible to achieve the parametric rate in the estimation of each entry . this lower bound result which provides an answer to the xxunk sample size question is</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xxbos a large scale structure traced by oii emitters hosting a distant cluster at xxunk we present a xxunk narrow band imaging survey of oii emitters in and around the xxunk xxunk cluster at z= with xxunk xxunk on xxunk telescope . oii emitters were identified on the basis of narrow band excesses and photometric redshifts . we discovered a huge xxunk structure with some xxunk traced by oii emitters and found that the xxunk xxunk cluster is embedded in an even larger super structure than the one reported previously . oii emitters were spectroscopically confirmed with the detection of h alpha and or o xxrep 3 i emission lines by xxunk observations . in the high density regions such as cluster core and xxunk star forming oii emitters show a high xxunk by a factor of more than compared to the field region . although the star formation activity</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xxbos xxunk based low delay live streaming using throughput predictions . recently http based adaptive streaming has become the de xxunk standard for video streaming over the internet . it allows xxunk to dynamically adapt media characteristics to network conditions in order to ensure a high quality of experience that is minimize xxunk xxunk while maximizing video quality at a reasonable level of quality changes . in the case of live streaming this task becomes particularly challenging due to the latency constraints . the challenge further increases if a xxunk uses a wireless network where the throughput is subject to considerable fluctuations . consequently live xxunk often exhibit xxunk of up to seconds . in the present work we introduce an adaptation algorithm for http based live streaming called lolypop low latency prediction based adaptation that is designed to operate with a transport latency of few seconds . to reach</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xxbos large eddy simulations of turbulent flow for grid to rod xxunk in nuclear reactors . the grid to rod xxunk gtrf problem in xxunk water reactors is a flow induced vibration problem that results in xxunk and failure of the fuel xxunk in nuclear xxunk . in order to understand the fluid dynamics of gtrf and to build an archival database of turbulence statistics for various configurations implicit large eddy simulations of time dependent single phase turbulent flow have been performed in xxunk and xxunk rod bundles with a single grid xxunk . to assess the computational mesh and resolution requirements a method for quantitative assessment of xxunk meshes with no slip walls is described . the calculations have been carried out using hydra th a thermal xxunk code developed at los xxunk for the xxunk for advanced simulation of light water reactors a united states xxunk of energy</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>xxbos observation of the extremely bright flare of the fsrq xxunk with h.e.s.s . ii . in june the flat spectrum radio quasar xxunk xxunk an extremely bright gamma ray flare with an increase of the flux above mev by a factor in less than day revealing an intrinsic variability timescale of minutes as detected by the fermi lat . we present results of target of opportunity observations with the h.e.s.s . experiment on this source over the nights around the peak of the outburst . the h.e.s.s . data were analysed with mono and stereo chains . thanks to the extreme brightness of the source at gev energies it was possible to obtain data from fermi lat strictly simultaneous to the h.e.s.s . observation . simultaneous and quasi simultaneous observations at optical and x ray energies were xxunk to reconstruct the multi wavelength spectrum xxunk to constrain theoretical models</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>xxbos search for an xxunk in sodium and calcium in the transmission spectrum of xxunk cancri e. xxunk the aim of this work is to search for an absorption signal from exospheric sodium xxunk and xxunk ionized calcium ca in the optical transmission spectrum of the hot xxunk super earth cancri e. although the current best fitting models to the planet mass and radius require a possible atmospheric component uncertainties in the radius exist making it possible that cancri e could be a hot xxunk planet without an atmosphere . high resolution r time series spectra of five transits of cancri e obtained with three different telescopes xxunk vlt harps eso xxunk m harps n xxunk were analysed . targeting the sodium d lines and the calcium h and k lines the potential planet exospheric signal was filtered out from the much stronger stellar and xxunk signals making use of</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>xxbos interpolating helicity spinors between the instant form and the light front form . we discuss the helicity spinors interpolating between the instant form dynamics ifd and the front form dynamics or the light front dynamics lfd and present the interpolating helicity amplitudes as well as their xxunk for the scattering of two fermions and the annihilation of fermion and anti fermion . we xxunk the interpolation between the two dynamics ifd and lfd by an interpolation angle and derive not only the generalized helicity spinors in the chiral representation that links naturally the two typical ifd vs. lfd helicity spinors but also the generalized xxunk transformation that relates these generalized helicity spinors to the usual dirac spinors . analyzing the directions of the particle momentum and spin with the variation of the interpolation angle we xxunk the whole xxunk of the generalized helicity xxunk between the usual xxunk xxunk</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>xxbos xxunk xxunk drifting objects using an iterative algorithm with a forward trajectory model . the task of determining the origin of a drifting object after it has been located is highly complex due to the uncertainties in drift properties and environmental forcing wind waves and surface currents . usually the origin is inferred by running a trajectory model stochastic or deterministic in reverse . however this approach has some xxunk drawbacks most notably the fact that many drifting objects go through nonlinear state changes xxunk e.g. xxunk oil or a xxunk xxunk . this makes it difficult to naively construct a reverse time trajectory model which xxunk predicts the xxunk possible time the object may have started drifting . we propose instead a different approach where the original forward trajectory model is xxunk xxunk while an iterative xxunk and selection process allows us to retain only those particles that</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moved-forestry",
   "metadata": {},
   "source": [
    "## Refactor into a Single Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-mayor",
   "metadata": {},
   "source": [
    "We can refactor all this and export it as a single class with two useful methods:\n",
    "\n",
    "- An initializer that will retrain a new model for *every* new instance. This is intended, since we do not know the training set ahead of time. One potential improvement here would be to continuously train on every new `.jsonl` file that comes in and save the weights, but there is not enough data for that here. \n",
    "- A `predict` method that takes in a sentence and returns a prediction by querying the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "thick-empty",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LSTMClassifier:\n",
    "    def __init__(self, json='data/train.jsonl', samples=5, metrics=[]):\n",
    "        self.path = json\n",
    "        self.df = pd.DataFrame(uniform_samples(json, samples))\n",
    "        self.dls = TextDataLoaders.from_df(self.df, path=json, text_col='text', label_col='label', valid_col=None, seq_len=50)\n",
    "        self.learn = text_classifier_learner(self.dls, AWD_LSTM, drop_mult=0.5, metrics=metrics)\n",
    "        self.learn.fine_tune(5, 5e-2)\n",
    "        \n",
    "    def predict(self, prompt):\n",
    "        pred = self.learn.predict(prompt)[0]\n",
    "        return 'NOT AI' if (pred == 'False') else 'AI'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlimited-apparatus",
   "metadata": {},
   "source": [
    "> Note: you might have to restart the notebook to clear GPU memory at this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "instructional-jason",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'out of plane effect on the superconductivity of sr2 xbaxcuo3+d with tc up to 98k. we comment on the paper published by w.b. gao q.q. liu l.x. yang y.yu f.y. li c.q. jin and s. uchida in phys. rev. b and give alternate explanations for the enhanced superconductivity. the enhanced onset tc of 98k observed upon substituting ba for sr is attributed to optimal oxygen ordering rather than to the increase in volume. comparison with la2cuo +x samples suggest that the effect of disorder is overestimated.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = load_jsonl(\"data/test_no_labels.jsonl\")\n",
    "example = test[0]\n",
    "prompt = example['text']\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "pregnant-dietary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iyaja/miniconda3/lib/python3.8/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.767748</td>\n",
       "      <td>0.685123</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.542596</td>\n",
       "      <td>1.003131</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.654398</td>\n",
       "      <td>0.495892</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.649306</td>\n",
       "      <td>0.447348</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.573689</td>\n",
       "      <td>0.476737</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.522111</td>\n",
       "      <td>0.466225</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.5 s, sys: 7.69 s, total: 18.2 s\n",
      "Wall time: 16.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clas = LSTMClassifier(metrics=[accuracy])\n",
    "pred = clas.predict(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "artistic-reynolds",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Not AI'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
