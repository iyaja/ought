{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp starter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ought Text Classification Project\n",
    "\n",
    "> Binary classification on scientific paper abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-2 Prompt Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest approach is to to feed in a small number of trainging examples into the prompt directly for zero-shot classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.eval().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def generate(prompt, max_length=5, stop_token=None):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    generated_text_ids = model.generate(input_ids=input_ids.cuda(), max_length=max_length+len(input_ids[0]), do_sample=False)\n",
    "    generated_text = tokenizer.decode(generated_text_ids[0], clean_up_tokenization_spaces=True)\n",
    "    post_prompt_text = generated_text[len(tokenizer.decode(input_ids[0], clean_up_tokenization_spaces=True)):]\n",
    "    return prompt + post_prompt_text[:post_prompt_text.find(stop_token) if stop_token else None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the logits are shifted over 1 to the left, since HuggingFace doesn't give a logit for the first token\n",
    "def get_logits_and_tokens(text):\n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    tokens = [tokenizer.decode([input_id]) for input_id in input_ids[0]]\n",
    "    output = model(input_ids.cuda())\n",
    "    return output.logits[0][:-1], tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Horrible: negative\\nGreat: positive\\nBad: negative'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXAMPLE_PROMPT = \"\"\"Horrible: negative\n",
    "Great: positive\n",
    "Bad:\"\"\"\n",
    "\n",
    "generated_text = generate(EXAMPLE_PROMPT, stop_token=\"\\n\")\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: ['Hor', 'rible', ':', ' negative', '\\n', 'Great', ':', ' positive', '\\n', 'Bad', ':', ' negative']\n",
      "negative prob: 0.7252255082130432\n",
      "positive prob: 0.11788686364889145\n"
     ]
    }
   ],
   "source": [
    "logits, tokens = get_logits_and_tokens(generated_text)\n",
    "last_token_probs = torch.softmax(logits[-1], dim=0)\n",
    "negative_prob = last_token_probs[tokenizer.encode(\" negative\")[0]]\n",
    "positive_prob = last_token_probs[tokenizer.encode(\" positive\")[0]]\n",
    "\n",
    "print(f\"tokens: {tokens}\\nnegative prob: {negative_prob}\\npositive prob: {positive_prob}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define helper function to load text from `.jsonl` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load_jsonl(filename):\n",
    "    f = open(filename)\n",
    "    return [json.loads(line) for line in f.read().splitlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 'False',\n",
       " 'text': 'non relativistic approach for cosmological scalar field dark matter. we derive non relativistic equations of motion for the formation of cosmological structure in a scalar field dark matter sfdm model corresponding to a complex scalar field endowed with a quadratic scalar potential. starting with the full equations of motion written in the newtonian gauge of scalar perturbations we separate out the fields involved into relativistic and non relativistic parts and find the equations of motion for the latter that can be used to build up the full solution. one important assumption will also be that the sfdm field is in the regime of fast oscillations under which its behavior is exactly that of cold dark matter. the resultant equations are quite similar to the schr odinger poisson system of newtonian boson stars plus relativistic leftovers. we exploit that similarity to show how to simulate with minimum numerical effort the formation of cosmological structure in sfdm models and others alike and ultimately prove their viability as complete dark matter models.',\n",
       " 'meta': {'id': '1310.8601', 'year': 2013}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_examples = load_jsonl(\"data/train.jsonl\")\n",
    "train_examples[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def render_example(example):\n",
    "    title = example[\"text\"].split(\".\")[0].strip()\n",
    "    abstract = example[\"text\"][len(title)+1:].strip()\n",
    "    return f'TITLE: {title}\\nABSTRACT: {abstract}\\nLABEL: {\"AI\" if example[\"label\"] == \"True\" else \"NOT AI\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def render_end_example(example):\n",
    "    title = example[\"text\"].split(\".\")[0].strip()\n",
    "    abstract = example[\"text\"][len(title)+1:].strip()\n",
    "    return f\"TITLE: {title}\\nABSTRACT: {abstract}\\nLABEL:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def make_prompt(instructions, train_examples, end_example):\n",
    "    rendered_train_examples = \"\\n\\n--\\n\\n\".join([render_example(example) for example in train_examples])\n",
    "    return f\"\"\"{instructions}\n",
    "\n",
    "{rendered_train_examples}\n",
    "\n",
    "--\n",
    "\n",
    "{render_end_example(end_example)}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify the following examples based on whether they are AI-relevant or not:\n",
      "\n",
      "TITLE: thermodynamic analysis of quantum error correcting engines\n",
      "ABSTRACT: quantum error correcting codes can be cast in a way which is strikingly similar to a quantum heat engine undergoing an otto cycle. in this paper we strengthen this connection further by carrying out a complete assessment of the thermodynamic properties of strokes operator based error correcting codes. this includes an expression for the entropy production in the cycle which as we show contains clear contributions stemming from the different sources of irreversibility. to illustrate our results we study a classical qubit error correcting code well suited for incoherent states and the qubit shor code capable of handling fully quantum states. we show that the work cost associated with the correction gate is directly associated with the heat introduced by the error. moreover the work cost associated with encoding decoding quantum information is always positive a fact which is related to the intrinsic irreversibility introduced by the noise. finally we find that correcting the coherent and thus genuinely quantum part of a quantum state introduces substantial modifications related to the hadamard gates required to encode and decode coherences.\n",
      "LABEL: NOT AI\n",
      "\n",
      "--\n",
      "\n",
      "TITLE: nlo qcd corrections to wzjj production at the lhc\n",
      "ABSTRACT: we present a summary of the first calculation of nlo qcd corrections to wzjj production with leptonic decays at the lhc. our results show that the next to leading order corrections reduce significantly the scale uncertainties.\n",
      "LABEL: NOT AI\n",
      "\n",
      "--\n",
      "\n",
      "TITLE: asymptotics for lipschitz percolation above tilted planes\n",
      "ABSTRACT: we consider lipschitz percolation in dimensions above planes tilted by an angle along one or several coordinate axes. in particular we are interested in the asymptotics of the critical probability as as well as our principal results show that the convergence of the critical probability to is polynomial as and in addition we identify the correct order of this polynomial convergence and in we also obtain the correct prefactor.\n",
      "LABEL: NOT AI\n",
      "\n",
      "--\n",
      "\n",
      "TITLE: the colored jones polynomials for bridge links\n",
      "ABSTRACT: kuperberg introduced web spaces for some lie algebras which are generalizations of the kauffman bracket skein module on a disk with marked points. we derive some formulas for and clasped web spaces by graphical calculus using skein theory. these formulas are colored version of skein relations twist formulas and bubble skein expansion formulas. we calculate the and colored jones polynomials of bridge knots and links explicitly using twist formulas.\n",
      "LABEL: NOT AI\n",
      "\n",
      "--\n",
      "\n",
      "TITLE: population mixtures and searches of lensed and extended quasars across photometric surveys\n",
      "ABSTRACT: wide field photometric surveys enable searches of rare yet interesting objects such as strongly lensed quasars or quasars with a bright host galaxy. past searches for lensed quasars based on their optical and near infrared properties have relied on photometric cuts and spectroscopic pre selection as in the sloan quasar lens search or neural networks applied to photometric samples. these methods rely on cuts in morphology and colours with the risk of losing many interesting objects due to scatter in their population properties restrictive training sets systematic uncertainties in catalog based magnitudes and survey to survey photometric variations. here we explore the performance of a gaussian mixture model to separate point like quasars quasars with an extended host and strongly lensed quasars using griz psf and model magnitudes and wise w1 w2. the choice of optical magnitudes is due to their presence in all current and upcoming releases of wide field surveys whereas uv information is not always available. we then assess the contamination from blue galaxies and the role of additional features such as w3 magnitudes or psf model terms as morphological information. as a demonstration we conduct a search in a random of the sdss footprint and we provide the catalog of the sdss object with the highest `lens score in our selection that survive visual inspection and are spectroscopically confirmed to host active nuclei. we inspect archival data and find images of objects in the hubble legacy archive including known lenses. the code and materials are available to facilitate follow up.\n",
      "LABEL:\n"
     ]
    }
   ],
   "source": [
    "INSTRUCTIONS = \"Classify the following examples based on whether they are AI-relevant or not:\"\n",
    "\n",
    "prompt = make_prompt(INSTRUCTIONS, train_examples[:4], train_examples[4])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify the following examples based on whether they are AI-relevant or not:\n",
      "\n",
      "TITLE: thermodynamic analysis of quantum error correcting engines\n",
      "ABSTRACT: quantum error correcting codes can be cast in a way which is strikingly similar to a quantum heat engine undergoing an otto cycle. in this paper we strengthen this connection further by carrying out a complete assessment of the thermodynamic properties of strokes operator based error correcting codes. this includes an expression for the entropy production in the cycle which as we show contains clear contributions stemming from the different sources of irreversibility. to illustrate our results we study a classical qubit error correcting code well suited for incoherent states and the qubit shor code capable of handling fully quantum states. we show that the work cost associated with the correction gate is directly associated with the heat introduced by the error. moreover the work cost associated with encoding decoding quantum information is always positive a fact which is related to the intrinsic irreversibility introduced by the noise. finally we find that correcting the coherent and thus genuinely quantum part of a quantum state introduces substantial modifications related to the hadamard gates required to encode and decode coherences.\n",
      "LABEL: NOT AI\n",
      "\n",
      "--\n",
      "\n",
      "TITLE: nlo qcd corrections to wzjj production at the lhc\n",
      "ABSTRACT: we present a summary of the first calculation of nlo qcd corrections to wzjj production with leptonic decays at the lhc. our results show that the next to leading order corrections reduce significantly the scale uncertainties.\n",
      "LABEL: NOT AI\n",
      "\n",
      "--\n",
      "\n",
      "TITLE: asymptotics for lipschitz percolation above tilted planes\n",
      "ABSTRACT: we consider lipschitz percolation in dimensions above planes tilted by an angle along one or several coordinate axes. in particular we are interested in the asymptotics of the critical probability as as well as our principal results show that the convergence of the critical probability to is polynomial as and in addition we identify the correct order of this polynomial convergence and in we also obtain the correct prefactor.\n",
      "LABEL: NOT AI\n",
      "\n",
      "--\n",
      "\n",
      "TITLE: the colored jones polynomials for bridge links\n",
      "ABSTRACT: kuperberg introduced web spaces for some lie algebras which are generalizations of the kauffman bracket skein module on a disk with marked points. we derive some formulas for and clasped web spaces by graphical calculus using skein theory. these formulas are colored version of skein relations twist formulas and bubble skein expansion formulas. we calculate the and colored jones polynomials of bridge knots and links explicitly using twist formulas.\n",
      "LABEL: NOT AI\n",
      "\n",
      "--\n",
      "\n",
      "TITLE: population mixtures and searches of lensed and extended quasars across photometric surveys\n",
      "ABSTRACT: wide field photometric surveys enable searches of rare yet interesting objects such as strongly lensed quasars or quasars with a bright host galaxy. past searches for lensed quasars based on their optical and near infrared properties have relied on photometric cuts and spectroscopic pre selection as in the sloan quasar lens search or neural networks applied to photometric samples. these methods rely on cuts in morphology and colours with the risk of losing many interesting objects due to scatter in their population properties restrictive training sets systematic uncertainties in catalog based magnitudes and survey to survey photometric variations. here we explore the performance of a gaussian mixture model to separate point like quasars quasars with an extended host and strongly lensed quasars using griz psf and model magnitudes and wise w1 w2. the choice of optical magnitudes is due to their presence in all current and upcoming releases of wide field surveys whereas uv information is not always available. we then assess the contamination from blue galaxies and the role of additional features such as w3 magnitudes or psf model terms as morphological information. as a demonstration we conduct a search in a random of the sdss footprint and we provide the catalog of the sdss object with the highest `lens score in our selection that survive visual inspection and are spectroscopically confirmed to host active nuclei. we inspect archival data and find images of objects in the hubble legacy archive including known lenses. the code and materials are available to facilitate follow up.\n",
      "LABEL: NOT AI\n"
     ]
    }
   ],
   "source": [
    "generated_text = generate(prompt, stop_token=\"\\n\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Helper Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some extra utility functions that were not defined in the original starter code, but were still useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load_jsonl(\"data/train.jsonl\")\n",
    "df = pd.DataFrame(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect some random samples across classes. This should be flexible enough to generalize beyong the `'AI'` and `'Not AI'` labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['False', 'True'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'False',\n",
       "  'text': 'thermodynamic analysis of quantum error correcting engines. quantum error correcting codes can be cast in a way which is strikingly similar to a quantum heat engine undergoing an otto cycle. in this paper we strengthen this connection further by carrying out a complete assessment of the thermodynamic properties of strokes operator based error correcting codes. this includes an expression for the entropy production in the cycle which as we show contains clear contributions stemming from the different sources of irreversibility. to illustrate our results we study a classical qubit error correcting code well suited for incoherent states and the qubit shor code capable of handling fully quantum states. we show that the work cost associated with the correction gate is directly associated with the heat introduced by the error. moreover the work cost associated with encoding decoding quantum information is always positive a fact which is related to the intrinsic irreversibility introduced by the noise. finally we find that correcting the coherent and thus genuinely quantum part of a quantum state introduces substantial modifications related to the hadamard gates required to encode and decode coherences.',\n",
       "  'meta': {'id': '1911.06354', 'year': 2019}},\n",
       " {'label': 'False',\n",
       "  'text': 'nlo qcd corrections to wzjj production at the lhc. we present a summary of the first calculation of nlo qcd corrections to wzjj production with leptonic decays at the lhc. our results show that the next to leading order corrections reduce significantly the scale uncertainties.',\n",
       "  'meta': {'id': '1310.4369', 'year': 2013}},\n",
       " {'label': 'True',\n",
       "  'text': 'norec the norwegian review corpus. this paper presents the norwegian review corpus norec created for training and evaluating models for document level sentiment analysis. the full text reviews have been collected from major norwegian news sources and cover a range of different domains including literature movies video games restaurants music and theater in addition to product reviews across a range of categories. each review is labeled with a manually assigned score of as provided by the rating of the original author. this first release of the corpus comprises more than reviews. it is distributed using the conll u format pre processed using udpipe along with a rich set of metadata. the work reported in this paper forms part of the sant initiative sentiment analysis for norwegian text a project seeking to provide resources and tools for sentiment analysis and opinion mining for norwegian. as resources for sentiment analysis have so far been unavailable for norwegian norec represents a highly valuable and sought after addition to norwegian language technology.',\n",
       "  'meta': {'id': '1710.05370', 'year': 2017}},\n",
       " {'label': 'True',\n",
       "  'text': 'a new local adaptive thresholding technique in binarization. image binarization is the process of separation of pixel values into two groups white as background and black as foreground. thresholding plays a major in binarization of images. thresholding can be categorized into global thresholding and local thresholding. in images with uniform contrast distribution of background and foreground like document images global thresholding is more appropriate. in degraded document images where considerable background noise or variation in contrast and illumination exists there exists many pixels that cannot be easily classified as foreground or background. in such cases binarization with local thresholding is more appropriate. this paper describes a locally adaptive thresholding technique that removes background by using local mean and mean deviation. normally the local mean computational time depends on the window size. our technique uses integral sum image as a prior processing to calculate local mean. it does not involve calculations of standard deviations as in other local adaptive techniques. this along with the fact that calculations of mean is independent of window size speed up the process as compared to other local thresholding techniques.',\n",
       "  'meta': {'id': '1201.5227', 'year': 2012}}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_per_label = 2\n",
    "samples = []\n",
    "for label in df['label'].unique():\n",
    "    group = df[df.label == label]\n",
    "    idxs = group.index[:samples_per_label]\n",
    "    samples += [train[idx] for idx in idxs]\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def uniform_samples(json='data/train.jsonl', n_samples=2):\n",
    "    superset = load_jsonl(json)\n",
    "    df = pd.DataFrame(superset)\n",
    "    samples = []\n",
    "    for label in df['label'].unique():\n",
    "        group = df[df.label == label]\n",
    "        idxs = group.index[:n_samples]\n",
    "        samples += [superset[idx] for idx in idxs]\n",
    "    return samples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
