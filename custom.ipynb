{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "handmade-telescope",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp custom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-agreement",
   "metadata": {},
   "source": [
    "# Custom Solutions\n",
    "\n",
    "> Deviating from the starter code to get better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becoming-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ought.starter import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understood-episode",
   "metadata": {},
   "source": [
    "## Zero Shot Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nearby-comfort",
   "metadata": {},
   "source": [
    "The starter code is helpful, but Huggingfaceface has built-in tools for zero shot classification. This also makes it easier to test different models (some may be trained on a larger amount of scientific text, which will be helpful)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "simple-surface",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/bart-large-mnli were not used when initializing BartModel: ['model.encoder.version', 'model.decoder.version']\n",
      "- This IS expected if you are initializing BartModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at facebook/bart-large-mnli were not used when initializing BartForSequenceClassification: ['model.encoder.version', 'model.decoder.version']\n",
      "- This IS expected if you are initializing BartForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "clas = pipeline(\"zero-shot-classification\", device=0)\n",
    "labels = [\"AI\", \"Not AI\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "otherwise-snowboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clas(prompt, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "union-connectivity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7244300842285156, 0.27556997537612915]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred['scores']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outer-liability",
   "metadata": {},
   "source": [
    "## Few-Shot Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-vertical",
   "metadata": {},
   "source": [
    "It should be possible to fine-tune at \"runtime\" with small number of examples from the training set.\n",
    "\n",
    "Since the total training time must be under a minute, the model *cannot* be unreasonably large like BERT or GPT. Considering this, we'll train much simpler and smaller models that are known to have better convergence properties. This is important because we don't what the \"training\" data is going to be ahead of time, since this is user-supplied. Also, the model needs to be able to generalize from a small number of examples, for which large transformers may not be the best option.\n",
    "\n",
    "`fastai` is well-suited for this rapid training where convergence across many samples with minimal configuration is more important than stictly obtaining the highest possible accuracy on the runtime training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "consolidated-kingston",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historic-richmond",
   "metadata": {},
   "source": [
    "`fastai`is easy to work with when you adhere to their `DataLoaders` format. So first, convert the JSON data into a pandas `DataFrame`\n",
    "\n",
    "> Note: `fastai` handles importing common libraries like `pandas` and `matplotlib` under the usual namespaces, which is why you won't see those here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
