{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecological-pizza",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accurate-authorization",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "\n",
    "> Generic utility functions to evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "digital-university",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ought.starter import *\n",
    "from ought.lstm import LSTMClassifier\n",
    "from ought.starter import GPTClassifier\n",
    "from ought.bart import BARTClassifier\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elder-headline",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-parent",
   "metadata": {},
   "source": [
    "One of the first orders of buisness is setting up a clear objective to optimize. Here, the goal is to get as high an accuracy as possible on text classification on the test set, so the metric is accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "informative-newport",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Metrics:\n",
    "    def __init__(self, train_path='data/train.jsonl', valid_path='data/dev.jsonl'):\n",
    "        self.train = load_jsonl(train_path)\n",
    "        self.valid = load_jsonl(valid_path)\n",
    "        print(f\"loaded {len(self.valid)} examples\")\n",
    "        \n",
    "    def accuracy(self, predict_func, samples=500):\n",
    "        INSTRUCTIONS = 'Label each of the following examples as \"AI\" or \"NOT AI\"'\n",
    "        \n",
    "        hits = []\n",
    "        for i, sample in enumerate(self.valid):\n",
    "            if i > samples: break\n",
    "            # prompt = make_prompt(INSTRUCTIONS, self.valid[i - 5:i], self.valid[i])\n",
    "            prompt = sample['text']\n",
    "            response = predict_func(prompt)\n",
    "            if (response.upper() == 'NOT AI'):\n",
    "                pred = 'False'\n",
    "            elif (response.upper() == 'AI'):\n",
    "                pred = 'True'\n",
    "            else:\n",
    "                print(f\"got invalid response: {response}\")\n",
    "                continue\n",
    "                \n",
    "            real = self.valid[i]['label']\n",
    "            hits.append(pred == real)\n",
    "        \n",
    "        return np.array(hits).sum() / len(hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "automatic-viewer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 500 examples\n"
     ]
    }
   ],
   "source": [
    "metrics = Metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "helpful-tucson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy(lambda _: 'Not AI')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integral-hotel",
   "metadata": {},
   "source": [
    "## GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-calvin",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%capture\n",
    "model = GPTClassifier()\n",
    "acc = metrics.accuracy(model.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "deluxe-nightlife",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ideal-selling",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "likely-appliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%capture\n",
    "model = LSTMClassifier()\n",
    "acc = metrics.accuracy(model.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thermal-albany",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twelve-cache",
   "metadata": {},
   "source": [
    "## BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-maine",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%capture\n",
    "model = BARTClassifier()\n",
    "acc = metrics.accuracy(model.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "random-membrane",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
