# AUTOGENERATED! DO NOT EDIT! File to edit: 02_metrics.ipynb (unless otherwise specified).

__all__ = ['Metrics']

# Cell
class Metrics:
    def __init__(self, train_path='data/train.jsonl', valid_path='data/dev.jsonl'):
        self.train = load_jsonl(train_path)
        self.valid = load_jsonl(valid_path)
        print(f"loaded {len(self.valid)} examples")

    def accuracy(self, predict_func, samples=500):
        INSTRUCTIONS = 'Label each of the following examples as "AI" or "NOT AI"'

        hits = []
        for i, sample in enumerate(self.valid):
            if i > samples: break
            # prompt = make_prompt(INSTRUCTIONS, self.valid[i - 5:i], self.valid[i])
            prompt = sample['text']
            response = predict_func(prompt)
            if (response.upper() == 'NOT AI'):
                pred = 'False'
            elif (response.upper() == 'AI'):
                pred = 'True'
            else:
                print(f"got invalid response: {response}")
                continue

            real = self.valid[i]['label']
            hits.append(pred == real)

        return np.array(hits).sum() / len(hits)