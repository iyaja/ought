# AUTOGENERATED! DO NOT EDIT! File to edit: 05_gpt.ipynb (unless otherwise specified).

__all__ = ['GPTBase', 'GPTLMClassifier', 'GPTMatmulClassifier', 'GPTSimilarityClassifier']

# Cell
from .starter import *
from transformers import GPT2Tokenizer, GPT2LMHeadModel
import torch
import json
import pandas as pd
import numpy as np

# Cell
class GPTBase:
    def __init__(self):
        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
        self.tokenizer.pad_token = self.tokenizer.eos_token
        self.model = GPT2LMHeadModel.from_pretrained('gpt2')
        self.model.eval().cuda()

    def generate(self, prompt, max_length=5, stop_token=None):
        input_ids = self.tokenizer.encode(prompt, return_tensors="pt")
        generated_text_ids = self.model.generate(input_ids=input_ids.cuda(), max_length=max_length+len(input_ids[0]), do_sample=False)
        generated_text = self.tokenizer.decode(generated_text_ids[0], clean_up_tokenization_spaces=True)
        post_prompt_text = generated_text[len(tokenizer.decode(input_ids[0], clean_up_tokenization_spaces=True)):]
        return prompt + post_prompt_text[:post_prompt_text.find(stop_token) if stop_token else None]

    def get_logits_and_tokens(self, text):
        input_ids = self.tokenizer.encode(text, return_tensors="pt")
        tokens = [self.tokenizer.decode([input_id]) for input_id in input_ids[0]]
        output = self.model(input_ids.cuda())
        return output.logits[0][:-1], tokens

# Cell
class GPTLMClassifier(GPTBase):
    def __init__(self, instructions='Label each of the following examples as "AI" or "NOT AI"', json='data/train.jsonl', samples=4):
        super(GPTLMClassifier, self).__init__()
        self.instructions = instructions
        self.context = load_jsonl(json)[:samples]

    def predict(self, prompt):
        prompt = make_prompt(self.instructions, self.context, {'text': prompt})
        out = self.generate(prompt, stop_token="\n")
        # to create a concrete prediction, take the last line and strip the "LABEL: " component
        pred = out.split('\n')[-1].strip("LABEL: ")
        return pred

# Cell
class GPTMatmulClassifier(GPTBase):
    def __init__(self, json='data/train.jsonl', samples_per_label=2):
        super(GPTMatmulClassifier, self).__init__()
        self.samples = uniform_samples(json, samples_per_label)
        self.outs = []
        for sample in self.samples:
            input_ids = self.tokenizer.encode(sample['text'], return_tensors="pt")
            tokens = [self.tokenizer.decode([input_id]) for input_id in input_ids[0]]
            out = self.model(input_ids.cuda(), output_hidden_states=True)
            self.outs.append(out)
        self.targs = [out.hidden_states[-1] for out in self.outs]

    def predict(self, prompt):

        input_ids = self.tokenizer.encode(prompt, return_tensors="pt")
        tokens = [self.tokenizer.decode([input_id]) for input_id in input_ids[0]]
        source = self.model(input_ids.cuda(), output_hidden_states=True).hidden_states[-1]

        scores = []
        for targ in self.targs:
            min_idx = min(source.size()[1], targ.size()[1])
            score = (source[0,:min_idx,:].T@targ[0,:min_idx,:]).norm() / min_idx
            scores.append(score)

        pred = self.samples[scores.index(max(scores))]['label']
        # this portion is specific to binary classification
        return 'NOT AI' if pred == 'False' else 'AI'


# Cell
class GPTSimilarityClassifier(GPTBase):
    def __init__(self, json='data/train.jsonl', samples=2):
        super(GPTSimilarityClassifier, self).__init__()
        self.samples = uniform_samples(json, samples)
        self.outs = []
        for sample in self.samples:
            input_ids = self.tokenizer.encode(sample['text'], return_tensors="pt")
            tokens = [self.tokenizer.decode([input_id]) for input_id in input_ids[0]]
            out = self.model(input_ids.cuda(), output_hidden_states=True)
            self.outs.append(out)
        self.targs = [out.hidden_states[-1] for out in self.outs]

    def predict(self, prompt):

        input_ids = self.tokenizer.encode(prompt, return_tensors="pt")
        tokens = [self.tokenizer.decode([input_id]) for input_id in input_ids[0]]
        source = self.model(input_ids.cuda(), output_hidden_states=True).hidden_states[-1]

        scores = []
        for targ in self.targs:
            min_idx = min(source.size()[1], targ.size()[1])
            score = (source[0,:min_idx,:] * targ[0,:min_idx,:]).sum() / min_idx
            scores.append(score)

        pred = self.samples[scores.index(max(scores))]['label']
        # this portion is specific to binary classification
        return 'NOT AI' if pred == 'False' else 'AI'