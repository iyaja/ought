{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from ought import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ought Text Classification\n",
    "\n",
    "> Few-shot text classification on scientific abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ought` package is currently not being distributed. But it can be installed locally with git:\n",
    "\n",
    "```\n",
    "git clone https://github.com/iyaja/ought.git\n",
    "cd ought\n",
    "pip install .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top-level `ought` module is plit into 5 main submodules:\n",
    "\n",
    "1. `starter`: The provided starter code, which contains a suite of utility functions for GPT-2\n",
    "2. `gpt`: Uses GPT-2 in multiple ways to expose 3 different classifiers.\n",
    "2. `lstm`: Implements LSTM models that train on a few examples at test time.\n",
    "2. `bart`: Implements BART-based classifier.\n",
    "2. `metrics`: Final metrics/scores for all models and ensemble classifier.\n",
    "\n",
    "All classification models provided are of the form `XXClassifier` where `XX` is the model name. Currently, the implemented classifiers are:\n",
    "\n",
    "- `GPTLMClassifier`: A GPT-2 language-model based classifier.\n",
    "- `GPTMatmulClassifier`: A few-shot shot GPT-2 classifier that computes prediction scores though matrix multiplication of stacked hidden states.\n",
    "- `GPTSimilarityClassifier`: A few-shot shot GPT-2 classifier that computes prediction scores though dot products of hidden states.\n",
    "- `LSTMClassifier`: A classifier that requires few-shot training using an LSTM. Training can be completed in under a minute for 500+ samples\n",
    "- `BARTClassifier`: A zero-shot classifier based on the BART architecure.\n",
    "\n",
    "All of the above models confirm to a standard interface. They all have an initializer that takes in a path to a `jsonl` file with context examples and a `samples` parameter to control how many xontextual samples to use per label/class. Once initialized, they all expose a `predict` method that takes in a string and returns either `\"AI\"` or `\"Not AI`. The results and accuracy for each model can be found in the `04_metrics.ipynb` notebook or, equaivalently, in the [documentation site](https://iyaja.github.io/ought/metrics.html).\n",
    "\n",
    "Here is an example of using the `GPTMatmulClassifier`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AI'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ought.gpt import GPTMatmulClassifier\n",
    "model = GPTMatmulClassifier(json='data/train.jsonl', samples=5)\n",
    "model.predict(\"In this paper, we propose a new zero-shot transformer-based algorithm to classifiy scientific papers as AI or NOT AI.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
