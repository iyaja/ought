{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "going-architecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp gpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspended-saint",
   "metadata": {},
   "source": [
    "# GPT Zero-Shot Classification\n",
    "\n",
    "> Attempting zero-shot solutions confined to just GPT-2 with no fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "different-raise",
   "metadata": {},
   "source": [
    "The other solutions perform well, and it's unlikely that GPT-2 alone will do better than BART, but it's worth a shot, and there are other interesting thigs we can try with the GPT architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-denial",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from ought.starter import *\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conceptual-forestry",
   "metadata": {},
   "source": [
    "Since all of these experiments will use GPT-2 in some way, it's useful to have a top-level base class that provides shared functionality to all the GPT variants. The base class itself does *not* implement the `predict` method. Note that is is mostly a cleaned up and packaged version of the starter code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rough-hazard",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class GPTBase:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "        self.model.eval().cuda()\n",
    "        \n",
    "    def generate(self, prompt, max_length=5, stop_token=None):\n",
    "        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "        generated_text_ids = self.model.generate(input_ids=input_ids.cuda(), max_length=max_length+len(input_ids[0]), do_sample=False)\n",
    "        generated_text = self.tokenizer.decode(generated_text_ids[0], clean_up_tokenization_spaces=True)\n",
    "        post_prompt_text = generated_text[len(self.tokenizer.decode(input_ids[0], clean_up_tokenization_spaces=True)):]\n",
    "        return prompt + post_prompt_text[:post_prompt_text.find(stop_token) if stop_token else None]\n",
    "    \n",
    "    def get_logits_and_tokens(self, text):\n",
    "        input_ids = self.tokenizer.encode(text, return_tensors=\"pt\")\n",
    "        tokens = [self.tokenizer.decode([input_id]) for input_id in input_ids[0]]\n",
    "        output = self.model(input_ids.cuda())\n",
    "        return output.logits[0][:-1], tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vertical-clerk",
   "metadata": {},
   "source": [
    "## Raw Language Model Token Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medical-bullet",
   "metadata": {},
   "source": [
    "This is the simplest possible solution - a replica of the starter code refactored into a single class to provide an interface that is consistent with the other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technical-operator",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class GPTLMClassifier(GPTBase):\n",
    "    def __init__(self, instructions='Label each of the following examples as \"AI\" or \"NOT AI\"', json='data/train.jsonl', samples=4):\n",
    "        super(GPTLMClassifier, self).__init__()\n",
    "        self.instructions = instructions\n",
    "        self.context = load_jsonl(json)[:samples]\n",
    "        \n",
    "    def predict(self, prompt):\n",
    "        prompt = make_prompt(self.instructions, self.context, {'text': prompt})\n",
    "        out = self.generate(prompt, stop_token=\"\\n\")\n",
    "        # to create a concrete prediction, take the last line and strip the \"LABEL: \" component \n",
    "        pred = out.split('\\n')[-1].strip(\"LABEL: \")\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suburban-waters",
   "metadata": {},
   "source": [
    "> Note: you might have to restart the notebook to clear GPU memory at this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distributed-shoulder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'out of plane effect on the superconductivity of sr2 xbaxcuo3+d with tc up to 98k. we comment on the paper published by w.b. gao q.q. liu l.x. yang y.yu f.y. li c.q. jin and s. uchida in phys. rev. b and give alternate explanations for the enhanced superconductivity. the enhanced onset tc of 98k observed upon substituting ba for sr is attributed to optimal oxygen ordering rather than to the increase in volume. comparison with la2cuo +x samples suggest that the effect of disorder is overestimated.'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = load_jsonl(\"data/test_no_labels.jsonl\")\n",
    "example = test[0]\n",
    "prompt = example['text']\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "homeless-sixth",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.2 s, sys: 333 ms, total: 10.6 s\n",
      "Wall time: 6.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clas = GPTLMClassifier()\n",
    "pred = clas.predict(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unknown-ecology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NOT AI'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "talented-sending",
   "metadata": {},
   "source": [
    "## Embedding Similarity Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binding-blowing",
   "metadata": {},
   "source": [
    "Another approach to classification would be to compare the final-layer embeddings of the unknown sample to that of known samples.\n",
    "\n",
    "The general hypothesis here is that you should know most of what you need to know about a paper . In other words, the marginal information provided by the next word in abstract decreases across the sequence. So, we collect the maximum number of hidden states possible and perform a matrix multiplication. The norm of the resulting matrix is the similarity score.True  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-mongolia",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = GPTBase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unnecessary-mattress",
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = []\n",
    "for sample in uniform_samples():    \n",
    "    input_ids = gpt.tokenizer.encode(sample['text'], return_tensors=\"pt\")\n",
    "    tokens = [gpt.tokenizer.decode([input_id]) for input_id in input_ids[0]]\n",
    "    output = gpt.model(input_ids.cuda(), output_hidden_states=True)\n",
    "    outs.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-grenada",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = outs[0].hidden_states[0]\n",
    "targ_1 = outs[1].hidden_states[0]\n",
    "targ_2 = outs[2].hidden_states[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-yukon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(867.2198, device='cuda:0', grad_fn=<CopyBackwards>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_idx = min(source.size()[1], targ_1.size()[1])\n",
    "(source[0,:min_idx,:].T@targ_1[0,:min_idx,:]).norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpha-tattoo",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1994.7556, device='cuda:0', grad_fn=<CopyBackwards>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_idx = min(source.size()[1], targ_2.size()[1])\n",
    "(source[0,:min_idx,:].T@targ_2[0,:min_idx,:]).norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neural-galaxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, targ in enumerate(targs):\n",
    "    min_idx = min(source.size()[1], targ.size()[1])\n",
    "    score = (source[0,:min_idx,:].T@targ[0,:min_idx,:]).norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advised-mumbai",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class GPTMatmulClassifier(GPTBase):\n",
    "    def __init__(self, json='data/train.jsonl', samples=2):\n",
    "        super(GPTMatmulClassifier, self).__init__()\n",
    "        self.samples = uniform_samples(json, samples)\n",
    "        self.outs = []\n",
    "        for sample in self.samples:    \n",
    "            input_ids = self.tokenizer.encode(sample['text'], return_tensors=\"pt\")\n",
    "            tokens = [self.tokenizer.decode([input_id]) for input_id in input_ids[0]]\n",
    "            out = self.model(input_ids.cuda(), output_hidden_states=True)\n",
    "            self.outs.append(out)\n",
    "        self.targs = [out.hidden_states[-1] for out in self.outs]\n",
    "        \n",
    "    def predict(self, prompt):\n",
    "        \n",
    "        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "        tokens = [self.tokenizer.decode([input_id]) for input_id in input_ids[0]]\n",
    "        source = self.model(input_ids.cuda(), output_hidden_states=True).hidden_states[-1]\n",
    "        \n",
    "        scores = []\n",
    "        for targ in self.targs:\n",
    "            min_idx = min(source.size()[1], targ.size()[1])\n",
    "            score = (source[0,:min_idx,:].T@targ[0,:min_idx,:]).norm() / min_idx\n",
    "            scores.append(score)\n",
    "\n",
    "        pred = self.samples[scores.index(max(scores))]['label']\n",
    "        # this portion is specific to binary classification \n",
    "        return 'NOT AI' if pred == 'False' else 'AI'\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "circular-schema",
   "metadata": {},
   "source": [
    "We can now test this new classifier in the usual way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progressive-concentrate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'out of plane effect on the superconductivity of sr2 xbaxcuo3+d with tc up to 98k. we comment on the paper published by w.b. gao q.q. liu l.x. yang y.yu f.y. li c.q. jin and s. uchida in phys. rev. b and give alternate explanations for the enhanced superconductivity. the enhanced onset tc of 98k observed upon substituting ba for sr is attributed to optimal oxygen ordering rather than to the increase in volume. comparison with la2cuo +x samples suggest that the effect of disorder is overestimated.'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = load_jsonl(\"data/test_no_labels.jsonl\")\n",
    "example = test[0]\n",
    "prompt = example['text']\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-production",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(53382.4570, device='cuda:0', grad_fn=<DivBackward0>), tensor(52856.9922, device='cuda:0', grad_fn=<DivBackward0>), tensor(56165.2070, device='cuda:0', grad_fn=<DivBackward0>), tensor(53237.0820, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "CPU times: user 9.31 s, sys: 304 ms, total: 9.62 s\n",
      "Wall time: 6.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clas = GPTMatmulClassifier()\n",
    "pred = clas.predict(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cognitive-lighting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AI'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abroad-cream",
   "metadata": {},
   "source": [
    "One issue with this is that  matrix multiplications do not accurately capture similary between sets of vectors. They are also more computationally expensive. An alternative is using a dot product between each of the vectors, which *does* measure similarity more directly. One concern with dot products might be that they'll give too much importance to the positions, but self-attention should mitigate that concern. All hidden vectors at all positions should have *some* information about the sequence as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-feedback",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class GPTSimilarityClassifier(GPTBase):\n",
    "    def __init__(self, json='data/train.jsonl', samples=2):\n",
    "        super(GPTSimilarityClassifier, self).__init__()\n",
    "        self.samples = uniform_samples(json, samples)\n",
    "        self.outs = []\n",
    "        for sample in self.samples:    \n",
    "            input_ids = self.tokenizer.encode(sample['text'], return_tensors=\"pt\")\n",
    "            tokens = [self.tokenizer.decode([input_id]) for input_id in input_ids[0]]\n",
    "            out = self.model(input_ids.cuda(), output_hidden_states=True)\n",
    "            self.outs.append(out)\n",
    "        self.targs = [out.hidden_states[-1] for out in self.outs]\n",
    "        \n",
    "    def predict(self, prompt):\n",
    "        \n",
    "        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "        tokens = [self.tokenizer.decode([input_id]) for input_id in input_ids[0]]\n",
    "        source = self.model(input_ids.cuda(), output_hidden_states=True).hidden_states[-1]\n",
    "        \n",
    "        scores = []\n",
    "        for targ in self.targs:\n",
    "            min_idx = min(source.size()[1], targ.size()[1])\n",
    "            score = (source[0,:min_idx,:] * targ[0,:min_idx,:]).sum() / min_idx\n",
    "            scores.append(score)\n",
    "\n",
    "        pred = self.samples[scores.index(max(scores))]['label']\n",
    "        # this portion is specific to binary classification \n",
    "        return 'NOT AI' if pred == 'False' else 'AI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disciplinary-process",
   "metadata": {},
   "outputs": [],
   "source": [
    "clas = GPTSimilarityClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-sheep",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(51208.2266, device='cuda:0', grad_fn=<DivBackward0>), tensor(49627.3359, device='cuda:0', grad_fn=<DivBackward0>), tensor(55836.4375, device='cuda:0', grad_fn=<DivBackward0>), tensor(55042.0508, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "CPU times: user 26.5 ms, sys: 94 Âµs, total: 26.6 ms\n",
      "Wall time: 26 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pred = clas.predict(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "victorian-stockholm",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AI'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
