---

title: GPT Zero-Shot Classification


keywords: fastai
sidebar: home_sidebar

summary: "Attempting zero-shot solutions confined to just GPT-2 with no fine-tuning"
description: "Attempting zero-shot solutions confined to just GPT-2 with no fine-tuning"
nb_path: "01_gpt.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: 01_gpt.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The other solutions perform well, and it's unlikely that GPT-2 alone will do better than BART, but it's worth a shot, and there are other interesting thigs we can try with the GPT architecture.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Since all of these experiments will use GPT-2 in some way, it's useful to have a top-level base class that provides shared functionality to all the GPT variants. The base class itself does <em>not</em> implement the <code>predict</code> method. Note that is is mostly a cleaned up and packaged version of the starter code.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="GPTBase" class="doc_header"><code>class</code> <code>GPTBase</code><a href="https://github.com/iyaja/ought/tree/main/ought/gpt.py#L14" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>GPTBase</code>()</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Raw-Language-Model-Token-Classifier">Raw Language Model Token Classifier<a class="anchor-link" href="#Raw-Language-Model-Token-Classifier"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This is the simplest possible solution - a replica of the starter code refactored into a single class to provide an interface that is consistent with the other models.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="GPTLMClassifier" class="doc_header"><code>class</code> <code>GPTLMClassifier</code><a href="https://github.com/iyaja/ought/tree/main/ought/gpt.py#L35" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>GPTLMClassifier</code>(<strong><code>instructions</code></strong>=<em><code>'Label each of the following examples as "AI" or "NOT AI"'</code></em>, <strong><code>json</code></strong>=<em><code>'data/train.jsonl'</code></em>, <strong><code>samples</code></strong>=<em><code>4</code></em>) :: <a href="/ought/gpt.html#GPTBase"><code>GPTBase</code></a></p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include note.html content='you might have to restart the notebook to clear GPU memory at this point' %}</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">test</span> <span class="o">=</span> <span class="n">load_jsonl</span><span class="p">(</span><span class="s2">&quot;data/test_no_labels.jsonl&quot;</span><span class="p">)</span>
<span class="n">example</span> <span class="o">=</span> <span class="n">test</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="n">example</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span>
<span class="n">prompt</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&#39;out of plane effect on the superconductivity of sr2 xbaxcuo3+d with tc up to 98k. we comment on the paper published by w.b. gao q.q. liu l.x. yang y.yu f.y. li c.q. jin and s. uchida in phys. rev. b and give alternate explanations for the enhanced superconductivity. the enhanced onset tc of 98k observed upon substituting ba for sr is attributed to optimal oxygen ordering rather than to the increase in volume. comparison with la2cuo +x samples suggest that the effect of disorder is overestimated.&#39;</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%%time</span>
<span class="n">clas</span> <span class="o">=</span> <span class="n">GPTLMClassifier</span><span class="p">()</span>
<span class="n">pred</span> <span class="o">=</span> <span class="n">clas</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>CPU times: user 10.2 s, sys: 333 ms, total: 10.6 s
Wall time: 6.78 s
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">pred</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&#39;NOT AI&#39;</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Embedding-Similarity-Score">Embedding Similarity Score<a class="anchor-link" href="#Embedding-Similarity-Score"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Another approach to classification would be to compare the final-layer embeddings of the unknown sample to that of known samples.</p>
<p>The general hypothesis here is that you should know most of what you need to know about a paper . In other words, the marginal information provided by the next word in abstract decreases across the sequence. So, we collect the maximum number of hidden states possible and perform a matrix multiplication. The norm of the resulting matrix is the similarity score.True</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">gpt</span> <span class="o">=</span> <span class="n">GPTBase</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">uniform_samples</span><span class="p">():</span>    
    <span class="n">input_ids</span> <span class="o">=</span> <span class="n">gpt</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">gpt</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">([</span><span class="n">input_id</span><span class="p">])</span> <span class="k">for</span> <span class="n">input_id</span> <span class="ow">in</span> <span class="n">input_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">gpt</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="o">.</span><span class="n">cuda</span><span class="p">(),</span> <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">source</span> <span class="o">=</span> <span class="n">outs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">targ_1</span> <span class="o">=</span> <span class="n">outs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">targ_2</span> <span class="o">=</span> <span class="n">outs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">min_idx</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">source</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">],</span> <span class="n">targ_1</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">])</span>
<span class="p">(</span><span class="n">source</span><span class="p">[</span><span class="mi">0</span><span class="p">,:</span><span class="n">min_idx</span><span class="p">,:]</span><span class="o">.</span><span class="n">T</span><span class="nd">@targ_1</span><span class="p">[</span><span class="mi">0</span><span class="p">,:</span><span class="n">min_idx</span><span class="p">,:])</span><span class="o">.</span><span class="n">norm</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor(867.2198, device=&#39;cuda:0&#39;, grad_fn=&lt;CopyBackwards&gt;)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">min_idx</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">source</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">],</span> <span class="n">targ_2</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">])</span>
<span class="p">(</span><span class="n">source</span><span class="p">[</span><span class="mi">0</span><span class="p">,:</span><span class="n">min_idx</span><span class="p">,:]</span><span class="o">.</span><span class="n">T</span><span class="nd">@targ_2</span><span class="p">[</span><span class="mi">0</span><span class="p">,:</span><span class="n">min_idx</span><span class="p">,:])</span><span class="o">.</span><span class="n">norm</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor(1994.7556, device=&#39;cuda:0&#39;, grad_fn=&lt;CopyBackwards&gt;)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">targ</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">targs</span><span class="p">):</span>
    <span class="n">min_idx</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">source</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">],</span> <span class="n">targ</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">score</span> <span class="o">=</span> <span class="p">(</span><span class="n">source</span><span class="p">[</span><span class="mi">0</span><span class="p">,:</span><span class="n">min_idx</span><span class="p">,:]</span><span class="o">.</span><span class="n">T</span><span class="nd">@targ</span><span class="p">[</span><span class="mi">0</span><span class="p">,:</span><span class="n">min_idx</span><span class="p">,:])</span><span class="o">.</span><span class="n">norm</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="GPTMatmulClassifier" class="doc_header"><code>class</code> <code>GPTMatmulClassifier</code><a href="https://github.com/iyaja/ought/tree/main/ought/gpt.py#L49" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>GPTMatmulClassifier</code>(<strong><code>json</code></strong>=<em><code>'data/train.jsonl'</code></em>, <strong><code>samples</code></strong>=<em><code>2</code></em>) :: <a href="/ought/gpt.html#GPTBase"><code>GPTBase</code></a></p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can now test this new classifier in the usual way.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">test</span> <span class="o">=</span> <span class="n">load_jsonl</span><span class="p">(</span><span class="s2">&quot;data/test_no_labels.jsonl&quot;</span><span class="p">)</span>
<span class="n">example</span> <span class="o">=</span> <span class="n">test</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="n">example</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span>
<span class="n">prompt</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&#39;out of plane effect on the superconductivity of sr2 xbaxcuo3+d with tc up to 98k. we comment on the paper published by w.b. gao q.q. liu l.x. yang y.yu f.y. li c.q. jin and s. uchida in phys. rev. b and give alternate explanations for the enhanced superconductivity. the enhanced onset tc of 98k observed upon substituting ba for sr is attributed to optimal oxygen ordering rather than to the increase in volume. comparison with la2cuo +x samples suggest that the effect of disorder is overestimated.&#39;</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%%time</span>
<span class="n">clas</span> <span class="o">=</span> <span class="n">GPTMatmulClassifier</span><span class="p">()</span>
<span class="n">pred</span> <span class="o">=</span> <span class="n">clas</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[tensor(53382.4570, device=&#39;cuda:0&#39;, grad_fn=&lt;DivBackward0&gt;), tensor(52856.9922, device=&#39;cuda:0&#39;, grad_fn=&lt;DivBackward0&gt;), tensor(56165.2070, device=&#39;cuda:0&#39;, grad_fn=&lt;DivBackward0&gt;), tensor(53237.0820, device=&#39;cuda:0&#39;, grad_fn=&lt;DivBackward0&gt;)]
CPU times: user 9.31 s, sys: 304 ms, total: 9.62 s
Wall time: 6.59 s
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">pred</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&#39;AI&#39;</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>One issue with this is that  matrix multiplications do not accurately capture similary between sets of vectors. They are also more computationally expensive. An alternative is using a dot product between each of the vectors, which <em>does</em> measure similarity more directly. One concern with dot products might be that they'll give too much importance to the positions, but self-attention should mitigate that concern. All hidden vectors at all positions should have <em>some</em> information about the sequence as a whole.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="GPTSimilarityClassifier" class="doc_header"><code>class</code> <code>GPTSimilarityClassifier</code><a href="https://github.com/iyaja/ought/tree/main/ought/gpt.py#L79" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>GPTSimilarityClassifier</code>(<strong><code>json</code></strong>=<em><code>'data/train.jsonl'</code></em>, <strong><code>samples</code></strong>=<em><code>2</code></em>) :: <a href="/ought/gpt.html#GPTBase"><code>GPTBase</code></a></p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">clas</span> <span class="o">=</span> <span class="n">GPTSimilarityClassifier</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%%time</span>
<span class="n">pred</span> <span class="o">=</span> <span class="n">clas</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[tensor(51208.2266, device=&#39;cuda:0&#39;, grad_fn=&lt;DivBackward0&gt;), tensor(49627.3359, device=&#39;cuda:0&#39;, grad_fn=&lt;DivBackward0&gt;), tensor(55836.4375, device=&#39;cuda:0&#39;, grad_fn=&lt;DivBackward0&gt;), tensor(55042.0508, device=&#39;cuda:0&#39;, grad_fn=&lt;DivBackward0&gt;)]
CPU times: user 26.5 ms, sys: 94 µs, total: 26.6 ms
Wall time: 26 ms
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">pred</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&#39;AI&#39;</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

